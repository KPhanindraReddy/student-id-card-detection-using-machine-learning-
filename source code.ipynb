{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2192596-75ec-4ad5-b229-0d4046ede615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into training and validation sets.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to split dataset\n",
    "def split_dataset(original_dir, train_dir, val_dir, split_ratio=0.8):\n",
    "    categories = os.listdir(original_dir)\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(original_dir, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        \n",
    "        images = os.listdir(category_path)\n",
    "        train_images, val_images = train_test_split(images, test_size=1-split_ratio)\n",
    "        \n",
    "        os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
    "        os.makedirs(os.path.join(val_dir, category), exist_ok=True)\n",
    "        \n",
    "        for img in train_images:\n",
    "            shutil.copy(os.path.join(category_path, img), os.path.join(train_dir, category, img))\n",
    "        \n",
    "        for img in val_images:\n",
    "            shutil.copy(os.path.join(category_path, img), os.path.join(val_dir, category, img))\n",
    "\n",
    "# Define paths\n",
    "original_dataset = \"dataset/original\"\n",
    "train_dir = \"dataset/train\"\n",
    "val_dir = \"dataset/validation\"\n",
    "\n",
    "# Create train and validation directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Split the dataset\n",
    "split_dataset(original_dataset, train_dir, val_dir)\n",
    "print(\"Dataset split into training and validation sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af47da36-3f3b-48aa-9192-9fe11194695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating images in dataset/train...\n",
      "Invalid image: dataset/train\\No_ID\\good-ideas-young-attractive-student.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\train\\\\No_ID\\\\good-ideas-young-attractive-student.jpg'\n",
      "Removed corrupted file: dataset/train\\No_ID\\good-ideas-young-attractive-student.jpg\n",
      "Invalid image: dataset/train\\No_ID\\happy-student-with-laptop.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\train\\\\No_ID\\\\happy-student-with-laptop.jpg'\n",
      "Removed corrupted file: dataset/train\\No_ID\\happy-student-with-laptop.jpg\n",
      "Invalid image: dataset/train\\No_ID\\student-wearing-a-backpack-on-white-background.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\train\\\\No_ID\\\\student-wearing-a-backpack-on-white-background.jpg'\n",
      "Removed corrupted file: dataset/train\\No_ID\\student-wearing-a-backpack-on-white-background.jpg\n",
      "Invalid image: dataset/train\\Wearing_ID\\confident-teacher-in-library.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\train\\\\Wearing_ID\\\\confident-teacher-in-library.jpg'\n",
      "Removed corrupted file: dataset/train\\Wearing_ID\\confident-teacher-in-library.jpg\n",
      "Invalid image: dataset/train\\Wearing_ID\\happy-teacher-smiling-at-the-university-while-using-a-digita.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\train\\\\Wearing_ID\\\\happy-teacher-smiling-at-the-university-while-using-a-digita.jpg'\n",
      "Removed corrupted file: dataset/train\\Wearing_ID\\happy-teacher-smiling-at-the-university-while-using-a-digita.jpg\n",
      "Invalid image: dataset/train\\Wearing_ID\\happy-young-man-smiling-at-camera.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\train\\\\Wearing_ID\\\\happy-young-man-smiling-at-camera.jpg'\n",
      "Removed corrupted file: dataset/train\\Wearing_ID\\happy-young-man-smiling-at-camera.jpg\n",
      "Invalid image: dataset/train\\Wearing_ID\\Identity-Card-Mockup-930x620.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\train\\\\Wearing_ID\\\\Identity-Card-Mockup-930x620.jpg'\n",
      "Removed corrupted file: dataset/train\\Wearing_ID\\Identity-Card-Mockup-930x620.jpg\n",
      "Invalid image: dataset/train\\Wearing_ID\\portrait-of-a-happy-teacher-smiling-at-the-university.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\train\\\\Wearing_ID\\\\portrait-of-a-happy-teacher-smiling-at-the-university.jpg'\n",
      "Removed corrupted file: dataset/train\\Wearing_ID\\portrait-of-a-happy-teacher-smiling-at-the-university.jpg\n",
      "Invalid image: dataset/train\\Wearing_ID\\portrait-of-female-delegate-during-break-at-conference.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\train\\\\Wearing_ID\\\\portrait-of-female-delegate-during-break-at-conference.jpg'\n",
      "Removed corrupted file: dataset/train\\Wearing_ID\\portrait-of-female-delegate-during-break-at-conference.jpg\n",
      "Invalid image: dataset/train\\Wearing_ID\\portrait-of-male-university-or-college-tutor-outdoors-with-m.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\train\\\\Wearing_ID\\\\portrait-of-male-university-or-college-tutor-outdoors-with-m.jpg'\n",
      "Removed corrupted file: dataset/train\\Wearing_ID\\portrait-of-male-university-or-college-tutor-outdoors-with-m.jpg\n",
      "Invalid image: dataset/train\\Wearing_ID\\portrait-of-smiling-female-school-teacher-standing-in-corrid.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\train\\\\Wearing_ID\\\\portrait-of-smiling-female-school-teacher-standing-in-corrid.jpg'\n",
      "Removed corrupted file: dataset/train\\Wearing_ID\\portrait-of-smiling-female-school-teacher-standing-in-corrid.jpg\n",
      "Invalid image: dataset/train\\Wearing_ID\\woman-wear-big-blank-white-vertical-badge-mockup.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\train\\\\Wearing_ID\\\\woman-wear-big-blank-white-vertical-badge-mockup.jpg'\n",
      "Removed corrupted file: dataset/train\\Wearing_ID\\woman-wear-big-blank-white-vertical-badge-mockup.jpg\n",
      "Validating images in dataset/validation...\n",
      "Invalid image: dataset/validation\\No_ID\\face-portrait-student-and-man-in-university-ready-for-back-t.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\validation\\\\No_ID\\\\face-portrait-student-and-man-in-university-ready-for-back-t.jpg'\n",
      "Removed corrupted file: dataset/validation\\No_ID\\face-portrait-student-and-man-in-university-ready-for-back-t.jpg\n",
      "Invalid image: dataset/validation\\Wearing_ID\\our-representative-will-call.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\validation\\\\Wearing_ID\\\\our-representative-will-call.jpg'\n",
      "Removed corrupted file: dataset/validation\\Wearing_ID\\our-representative-will-call.jpg\n",
      "Invalid image: dataset/validation\\Wearing_ID\\portrait-of-male-university-or-college-tutor-outdoors-with-m (1).jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\validation\\\\Wearing_ID\\\\portrait-of-male-university-or-college-tutor-outdoors-with-m (1).jpg'\n",
      "Removed corrupted file: dataset/validation\\Wearing_ID\\portrait-of-male-university-or-college-tutor-outdoors-with-m (1).jpg\n",
      "Invalid image: dataset/validation\\Wearing_ID\\快樂的年輕人微笑在相機.jpg - cannot identify image file 'C:\\\\Users\\\\Phani\\\\Downloads\\\\dataset\\\\validation\\\\Wearing_ID\\\\快樂的年輕人微笑在相機.jpg'\n",
      "Removed corrupted file: dataset/validation\\Wearing_ID\\快樂的年輕人微笑在相機.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Directories for your dataset\n",
    "directories = ['dataset/train', 'dataset/validation']\n",
    "\n",
    "def validate_images(directory):\n",
    "    for sub_dir in os.listdir(directory):\n",
    "        sub_dir_path = os.path.join(directory, sub_dir)\n",
    "        if not os.path.isdir(sub_dir_path):\n",
    "            continue\n",
    "        \n",
    "        for file_name in os.listdir(sub_dir_path):\n",
    "            file_path = os.path.join(sub_dir_path, file_name)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    img.verify()  # Verify the image integrity\n",
    "            except Exception as e:\n",
    "                print(f\"Invalid image: {file_path} - {e}\")\n",
    "                os.remove(file_path)  # Remove the invalid file\n",
    "                print(f\"Removed corrupted file: {file_path}\")\n",
    "\n",
    "# Validate train and validation directories\n",
    "for directory in directories:\n",
    "    print(f\"Validating images in {directory}...\")\n",
    "    validate_images(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a14491-2169-4686-a86c-7e682b916b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting webp files in dataset/train...\n",
      "Converting webp files in dataset/validation...\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def convert_webp_to_jpeg(directory):\n",
    "    for sub_dir in os.listdir(directory):\n",
    "        sub_dir_path = os.path.join(directory, sub_dir)\n",
    "        if not os.path.isdir(sub_dir_path):\n",
    "            continue\n",
    "        \n",
    "        for file_name in os.listdir(sub_dir_path):\n",
    "            file_path = os.path.join(sub_dir_path, file_name)\n",
    "            if file_name.endswith('.webp'):\n",
    "                try:\n",
    "                    img = Image.open(file_path).convert('RGB')  # Convert to RGB for compatibility\n",
    "                    new_file_path = file_path.rsplit('.', 1)[0] + '.jpg'\n",
    "                    img.save(new_file_path, 'JPEG')\n",
    "                    os.remove(file_path)  # Remove the original .webp file\n",
    "                    print(f\"Converted and replaced: {file_path} with {new_file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting {file_path}: {e}\")\n",
    "\n",
    "# Directories to check and convert\n",
    "directories = ['dataset/train', 'dataset/validation']\n",
    "\n",
    "for directory in directories:\n",
    "    print(f\"Converting webp files in {directory}...\")\n",
    "    convert_webp_to_jpeg(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e6a6410-e951-4239-ab42-5a4e41e08309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file types in dataset/train...\n",
      "Checking file types in dataset/validation...\n"
     ]
    }
   ],
   "source": [
    "import mimetypes\n",
    "\n",
    "def check_file_types(directory):\n",
    "    for sub_dir in os.listdir(directory):\n",
    "        sub_dir_path = os.path.join(directory, sub_dir)\n",
    "        if not os.path.isdir(sub_dir_path):\n",
    "            continue\n",
    "        \n",
    "        for file_name in os.listdir(sub_dir_path):\n",
    "            file_path = os.path.join(sub_dir_path, file_name)\n",
    "            mime_type, _ = mimetypes.guess_type(file_path)\n",
    "            if mime_type not in ['image/jpeg', 'image/png']:\n",
    "                print(f\"Invalid file type: {file_path} - {mime_type}\")\n",
    "                os.remove(file_path)\n",
    "                print(f\"Removed invalid file: {file_path}\")\n",
    "\n",
    "# Check file types\n",
    "for directory in ['dataset/train', 'dataset/validation']:\n",
    "    print(f\"Checking file types in {directory}...\")\n",
    "    check_file_types(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6cb7c52-7094-4a21-a6f4-3a16a8e840a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 239 images belonging to 2 classes.\n",
      "Found 60 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phani\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.6715 - loss: 3.7885 - val_accuracy: 0.7000 - val_loss: 5.6012\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.7479 - loss: 4.7399 - val_accuracy: 0.9167 - val_loss: 0.5503\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.8853 - loss: 2.1170 - val_accuracy: 0.8833 - val_loss: 1.5428\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.8475 - loss: 1.4914 - val_accuracy: 0.9000 - val_loss: 0.8233\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.8333 - loss: 1.6101 - val_accuracy: 0.9167 - val_loss: 0.5401\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.9296 - loss: 0.8174 - val_accuracy: 0.9167 - val_loss: 0.4888\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.9386 - loss: 0.3390 - val_accuracy: 0.9333 - val_loss: 0.3494\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.9069 - loss: 0.8795 - val_accuracy: 0.9333 - val_loss: 0.3393\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.8638 - loss: 0.7435 - val_accuracy: 0.9333 - val_loss: 0.3676\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.9002 - loss: 0.4510 - val_accuracy: 0.9167 - val_loss: 0.3629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as id_card_detection_model.h5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "# Directories for training and validation data\n",
    "train_dir = 'dataset/train'\n",
    "val_dir = 'dataset/validation'\n",
    "\n",
    "# Image preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load pre-trained MobileNetV2 model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(2, activation='softmax')(x)  # 2 classes: Wearing_ID and No_ID\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('id_card_detection_model.h5')\n",
    "print(\"Model saved as id_card_detection_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3892e5ae-21de-4cc2-8963-59e51c61be2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('id_card_detection_model.h5')\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c253715-c93b-415b-8bef-de8133636708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('id_card_detection_model.h5')\n",
    "\n",
    "# Load a pre-trained face detection model\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default camera\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray_frame,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30)\n",
    "    )\n",
    "\n",
    "    # Check if any faces are detected\n",
    "    if len(faces) > 0:\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract the face region\n",
    "            face = frame[y:y+h, x:x+w]\n",
    "\n",
    "            # Preprocess the face for the model\n",
    "            resized_face = cv2.resize(face, (224, 224))  # Resize to match model input\n",
    "            normalized_face = resized_face / 255.0  # Normalize pixel values\n",
    "            input_data = np.expand_dims(normalized_face, axis=0)  # Add batch dimension\n",
    "\n",
    "            # Make predictions\n",
    "            predictions = model.predict(input_data, verbose=0)\n",
    "            class_index = np.argmax(predictions)\n",
    "            class_label = \"Wearing ID\" if class_index == 1 else \"No ID\"\n",
    "\n",
    "            # Define the rectangle color\n",
    "            box_color = (0, 255, 0) if class_index == 1 else (0, 0, 255)  # Green for \"Wearing ID\", Red for \"No ID\"\n",
    "\n",
    "            # Draw a rectangle around the face\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), box_color, 2)\n",
    "\n",
    "            # Display the label near the face\n",
    "            cv2.putText(frame, class_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, box_color, 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"ID Card Detection\", frame)\n",
    "\n",
    "    # Exit loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3c41c7-4fd1-4228-8873-af0b6a435a70",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Count the number of samples in each class\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_counts \u001b[38;5;241m=\u001b[39m train_generator\u001b[38;5;241m.\u001b[39mclasses\n\u001b[0;32m      3\u001b[0m unique, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(train_counts, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(unique, counts)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# Count the number of samples in each class\n",
    "train_counts = train_generator.classes\n",
    "unique, counts = np.unique(train_counts, return_counts=True)\n",
    "print(\"Class distribution:\", dict(zip(unique, counts)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
